Unsupervised Learning
========================================================
author: Wim van der Ham
autosize: true

Overview
========================================================

![Overview](./model_schema.jpg)

Unsupervised Learning
========================================================

> Unsupervised learning is used when you do not want to predict anything but want to understand your data better. It is much harder than supervised learning because there is no simple goal.

It answers questions like:

- Is there an informative way to visualize the data? 
- Can we discover subgroups among the variables or among the observations?

Unsupervised Learning
========================================================

In this chapter we will discuss two different techniques:

1. [principal components analysis](https://en.wikipedia.org/wiki/Principal_component_analysis), a tool
used for data visualization or data pre-processing
1. [clustering](https://en.wikipedia.org/wiki/Cluster_analysis), a broad class of methods for discovering
unknown subgroups in data

PCA
========================================================

> Allows to summarize a data set with less variables that still explin most of the variation in the data

When the number of variables in the dataset is big it van be difficult to visualize the data. PCA reduces the number of variables while stil showing most of the variation in the data.

PCA - Example Graph
========================================================

![Two Principal Components Directed allong the Greates Variation](./pca.jpg)

PCA - USA Arrest Example
========================================================

Scaling and centering the variables around zero is required to calculate unbiased components

```{r}
library(tidyverse)
pca <- prcomp(
  USArrests, 
  center = TRUE, 
  scale. = TRUE
)
```

Plotting the Result - Code
========================================================

```{r, eval = FALSE}
library(ggfortify)
autoplot(
  pca, shape = FALSE, 
  label.size = 3,
  loadings = TRUE, loadings.colour = 'blue',
  loadings.label = TRUE, 
  loadings.label.size = 3
)
```

Plotting the Result - Graph
========================================================

```{r, echo=FALSE}
library(ggfortify)
autoplot(
  pca, shape = FALSE, 
  label.size = 3,
  loadings = TRUE, loadings.colour = 'blue',
  loadings.label = TRUE, 
  loadings.label.size = 3
)
```

Clustering
========================================================

> Make groups of observations that are similar to each other

Here we will explain two methods:

1. [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), devides the data in a pre-defined number of clusters
1. [Hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering), creates a dendrogram

k-means clustering
========================================================

1. Randomly assign every point to a cluster, the number of clusters is defined by the user
1. Repeat the next steps untill the centers are stable
  - Update the locations of the centers by taking the average of all assigned points
  - Assign each point to the closest (using the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance)) center for that point

k-means clustering - Wine Example
========================================================  

```{r}
library(rattle)
kc <- kmeans(wine, 3)
kc
```

Confusion Matrix
========================================================  

```{r}
table(wine$Type, kc$cluster)
```

Graph of Clusters
========================================================  

```{r}
wine %>%
  mutate(cluster = as.factor(kc$cluster)) %>%
  ggplot() +
  geom_point(
    aes(Alcohol, Malic, color = cluster)
  )
```

Dendogram mtcars example
========================================================

```{r}
# prepare hierarchical cluster
hc = hclust(dist(mtcars))
# very simple dendrogram
plot(hc, hang = -1)
```

Exercise
========================================================

1. Plot the first two principal components for the `iris` dataset and explain what for information you can get from this graph.
1. Try to cluster the `iris` dataset and check if it comes close to the actual `Species` label.